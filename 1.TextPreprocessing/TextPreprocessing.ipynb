{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "italian-universe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "verbal-fiber",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the first text file\n",
    "handle = open(\"PracticalText.txt\", \"r\", encoding=\"utf8\")\n",
    "PracFile = handle.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "labeled-settle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Conversely', ',', 'it', 'has', 'been', 'argued', 'that', 'these', 'huge', 'leaps', 'in', 'NLP', 'were', 'made', 'as', 'a', 'result', 'of', 'a', 'focus', 'shift', 'from', '“', 'rationalist', '”', 'methods', 'to', '“', 'empirical', '”', 'or', '“', 'corpus-based', '”', 'methods', '(', 'Brill', '&', 'Mooney', ',', '1997', ',', 'p.', '1', ')', '.', 'The', 'former', 'detailing', 'a', 'very', 'hand-coded', 'method', 'which', 'required', 'a', 'lot', 'of', 'self-observation', 'as', 'opposed', 'to', 'the', 'empirical', 'method', '.']\n"
     ]
    }
   ],
   "source": [
    "# Question 1 (a)\n",
    "\n",
    "tokenizingPracFile = word_tokenize(PracFile)\n",
    "print(tokenizingPracFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "oriental-silence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['conversely', 'argued', 'huge', 'leaps', 'nlp', 'made', 'result', 'focus', 'shift', '“', 'rationalist', '”', 'methods', '“', 'empirical', '”', '“', 'corpus-based', '”', 'methods', 'brill', '&', 'mooney', '1997', 'p', '1', 'former', 'detailing', 'hand-coded', 'method', 'required', 'lot', 'self-observation', 'opposed', 'empirical', 'method']\n"
     ]
    }
   ],
   "source": [
    "# Question 1 (b)\n",
    "def normalization(raw_d):\n",
    "    to_normalise = PracFile\n",
    "    to_normalise_lower = to_normalise.lower()\n",
    "    to_normalise_r = to_normalise_lower.replace(\",\", \"\").replace(\"'\", \"\").replace(\";\", \"\").replace(\".\",\"\").replace('\"', \"\").replace('\"', '').replace(\"(\", \"\").replace(\")\", \"\")\n",
    "    to_word_tokenised = word_tokenize(to_normalise_r)\n",
    "    #remove stop words\n",
    "    sw = set(stopwords.words('english'))\n",
    "    filtered_text = [i for i in to_word_tokenised if not i in sw]\n",
    "    print(filtered_text)\n",
    "    return filtered_text\n",
    "normalised_final = normalization(tokenizingPracFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "divine-graduate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# contractions_re = re.compile('(%s)'%'|'.join(contractions_dict.keys()))\n",
    "# def expand_contractions(s, contractions_dict=contractions_dict):\n",
    "#   def replace(match):\n",
    "#     return contractions_dict[match.group(0)]\n",
    "#   return contractions_re.sub(replace, s)\n",
    "# sentence = expand_contractions(sentence)\n",
    "# print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "senior-columbus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('conversely', 'RB'), ('argued', 'VBD'), ('huge', 'JJ'), ('leaps', 'NNS'), ('nlp', 'RB'), ('made', 'VBD'), ('result', 'NN'), ('focus', 'NN'), ('shift', 'NN'), ('“', 'NNP'), ('rationalist', 'NN'), ('”', 'NNP'), ('methods', 'NNS'), ('“', 'NNP'), ('empirical', 'JJ'), ('”', 'NNP'), ('“', 'NNP'), ('corpus-based', 'JJ'), ('”', 'NNP'), ('methods', 'NNS'), ('brill', 'NN'), ('&', 'CC'), ('mooney', 'NN'), ('1997', 'CD'), ('p', 'NN'), ('1', 'CD'), ('former', 'JJ'), ('detailing', 'VBG'), ('hand-coded', 'JJ'), ('method', 'NN'), ('required', 'VBN'), ('lot', 'RB'), ('self-observation', 'NN'), ('opposed', 'JJ'), ('empirical', 'JJ'), ('method', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# Question 1 (c)\n",
    "file_postag = pos_tag(normalised_final)\n",
    "print(file_postag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "analyzed-contemporary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The initial phase, tokenization involves breaking string into tokens that can be used. Formally, it can be defined as “defines a commonly accepted standard of clitic tokenization: separating conjunctions, affixival prepositions and pronouns, future marker clitics, and definite articles.” (Diab, 2009, p. 2) In layman’s terms, it could be thought of as a kind of delimiter.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the second text file\n",
    "file2 = open(\"PracticalText2.txt\", \"r\", encoding=\"utf8\")\n",
    "PracFile2 = file2.read()\n",
    "PracFile2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "interested-valve",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'the'), ('initial', 'initi'), ('phase', 'phase'), (',', ','), ('tokenization', 'token'), ('involves', 'involv'), ('breaking', 'break'), ('string', 'string'), ('into', 'into'), ('tokens', 'token'), ('that', 'that'), ('can', 'can'), ('be', 'be'), ('used', 'use'), ('.', '.'), ('Formally', 'formal'), (',', ','), ('it', 'it'), ('can', 'can'), ('be', 'be'), ('defined', 'defin'), ('as', 'as'), ('“', '“'), ('defines', 'defin'), ('a', 'a'), ('commonly', 'commonli'), ('accepted', 'accept'), ('standard', 'standard'), ('of', 'of'), ('clitic', 'clitic'), ('tokenization', 'token'), (':', ':'), ('separating', 'separ'), ('conjunctions', 'conjunct'), (',', ','), ('affixival', 'affixiv'), ('prepositions', 'preposit'), ('and', 'and'), ('pronouns', 'pronoun'), (',', ','), ('future', 'futur'), ('marker', 'marker'), ('clitics', 'clitic'), (',', ','), ('and', 'and'), ('definite', 'definit'), ('articles.', 'articles.'), ('”', '”'), ('(', '('), ('Diab', 'diab'), (',', ','), ('2009', '2009'), (',', ','), ('p.', 'p.'), ('2', '2'), (')', ')'), ('In', 'in'), ('layman', 'layman'), ('’', '’'), ('s', 's'), ('terms', 'term'), (',', ','), ('it', 'it'), ('could', 'could'), ('be', 'be'), ('thought', 'thought'), ('of', 'of'), ('as', 'as'), ('a', 'a'), ('kind', 'kind'), ('of', 'of'), ('delimiter', 'delimit'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# Question 2 (a)\n",
    "tokenizing2 = word_tokenize(PracFile2)\n",
    "ps = PorterStemmer()\n",
    "listtoken = []\n",
    "\n",
    "for w in tokenizing2:\n",
    "    listtoken.append((w, ps.stem(w)))\n",
    "print(listtoken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fallen-borough",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'The'), ('initial', 'initial'), ('phase', 'phase'), (',', ','), ('tokenization', 'tokenization'), ('involves', 'involves'), ('breaking', 'breaking'), ('string', 'string'), ('into', 'into'), ('tokens', 'token'), ('that', 'that'), ('can', 'can'), ('be', 'be'), ('used', 'used'), ('.', '.'), ('Formally', 'Formally'), (',', ','), ('it', 'it'), ('can', 'can'), ('be', 'be'), ('defined', 'defined'), ('as', 'a'), ('“', '“'), ('defines', 'defines'), ('a', 'a'), ('commonly', 'commonly'), ('accepted', 'accepted'), ('standard', 'standard'), ('of', 'of'), ('clitic', 'clitic'), ('tokenization', 'tokenization'), (':', ':'), ('separating', 'separating'), ('conjunctions', 'conjunction'), (',', ','), ('affixival', 'affixival'), ('prepositions', 'preposition'), ('and', 'and'), ('pronouns', 'pronoun'), (',', ','), ('future', 'future'), ('marker', 'marker'), ('clitics', 'clitics'), (',', ','), ('and', 'and'), ('definite', 'definite'), ('articles.', 'articles.'), ('”', '”'), ('(', '('), ('Diab', 'Diab'), (',', ','), ('2009', '2009'), (',', ','), ('p.', 'p.'), ('2', '2'), (')', ')'), ('In', 'In'), ('layman', 'layman'), ('’', '’'), ('s', 's'), ('terms', 'term'), (',', ','), ('it', 'it'), ('could', 'could'), ('be', 'be'), ('thought', 'thought'), ('of', 'of'), ('as', 'a'), ('a', 'a'), ('kind', 'kind'), ('of', 'of'), ('delimiter', 'delimiter'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# Question 2 (b)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "listlemmatizer = []\n",
    "for w in tokenizing2:\n",
    "    listlemmatizer.append((w, lemmatizer.lemmatize(w)))\n",
    "print(listlemmatizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "established-gossip",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'separate'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"separating\", pos = \"v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "single-folder",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper(nltk_tag):\n",
    "    if nltk_tag.startswith('J'): return 'a'\n",
    "    elif nltk_tag.startswith('V'): return 'v'\n",
    "    elif nltk_tag.startswith('N'): return 'n'    \n",
    "    elif nltk_tag.startswith('R'): return 'r'\n",
    "    else: return 'n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "timely-examination",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'n'), ('initial', 'a'), ('phase', 'n'), (',', 'n'), ('tokenization', 'n'), ('involve', 'v'), ('break', 'v'), ('string', 'v'), ('into', 'n'), ('token', 'n'), ('that', 'n'), ('can', 'n'), ('be', 'v'), ('use', 'v'), ('.', 'n'), ('Formally', 'r'), (',', 'n'), ('it', 'n'), ('can', 'n'), ('be', 'v'), ('define', 'v'), ('a', 'n'), ('“', 'a'), ('define', 'v'), ('a', 'n'), ('commonly', 'r'), ('accepted', 'a'), ('standard', 'n'), ('of', 'n'), ('clitic', 'a'), ('tokenization', 'n'), (':', 'n'), ('separating', 'n'), ('conjunction', 'n'), (',', 'n'), ('affixival', 'a'), ('preposition', 'n'), ('and', 'n'), ('pronoun', 'n'), (',', 'n'), ('future', 'a'), ('marker', 'n'), ('clitics', 'n'), (',', 'n'), ('and', 'n'), ('definite', 'a'), ('articles.', 'n'), ('”', 'n'), ('(', 'n'), ('Diab', 'n'), (',', 'n'), ('2009', 'n'), (',', 'n'), ('p.', 'v'), ('2', 'n'), (')', 'n'), ('In', 'n'), ('layman', 'n'), ('’', 'n'), ('s', 'n'), ('term', 'n'), (',', 'n'), ('it', 'n'), ('could', 'n'), ('be', 'v'), ('think', 'v'), ('of', 'n'), ('a', 'n'), ('a', 'n'), ('kind', 'n'), ('of', 'n'), ('delimiter', 'n'), ('.', 'n')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Q2. (b)\n",
    "'''Tokenize the new text-file and then lemmatize it using WordNet Lemmatizer; \n",
    "note you may have to pos-tag the sentences first and then convert the tags to make this work. Report the result of these steps and point out some of the things that look wrong.\n",
    "'''\n",
    "\n",
    "#print(f_two_pos_tagged)\n",
    "'''\n",
    "    wordnet_tagged = []\n",
    "    for tup_ele in f_two_pos_tagged:\n",
    "        wordnet_tagged.append([tup_ele[0], mapper(tup_ele[1])])\n",
    "    \n",
    "    lemmatized = []\n",
    "    for i in range(len(wordnet_tagged)):\n",
    "        k, v = wordnet_tagged[i][0], wordnet_tagged[i][1]\n",
    "        if v is not None:\n",
    "            lemmatized.append(lemmatizer.lemmatize(k, v))\n",
    "    print(lemmatized)\n",
    "    '''\n",
    "\n",
    "def lemmatize_(tokenizing2):\n",
    "    if tokenizing2 == None:\n",
    "        return None\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    f_two_pos_tagged = pos_tag(tokenizing2) #nltk pos tagged\n",
    "    wordnet_tagged = map(lambda ele: (ele[0], mapper(ele[1])), f_two_pos_tagged)\n",
    "    ls = []\n",
    "    for k, v in wordnet_tagged:\n",
    "        if v is not None:\n",
    "            ls.append((lemmatizer.lemmatize(k, v), v))\n",
    "    print(ls)\n",
    "lemmatize_(tokenizing2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
