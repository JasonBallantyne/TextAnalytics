{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "concerned-things",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import agreement\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from scipy.stats.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "superior-milan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3999999999999999\n",
      "0.0\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "opinions = ['Samsung has great screen quality and battery', \n",
    "            'Battery life on a OnePlus is better than any smart phone',\n",
    "            'The iPhone camera quality is better than any Android phone'\n",
    "           ]\n",
    "# +ve => 1; \n",
    "# -ve and neutral => 0\n",
    "raters = [[1, 0, 1],[1, -1, 1],[1, 0, -1]]\n",
    "\n",
    "# raters = [[\"Positive\", \"Neutral\", \"Positive\"],[\"Positive\", \"Negative\", \"Positive\"],[\"Positive\", \"Neutral\", \"Negative\"]]\n",
    "print(cohen_kappa_score(raters[0], raters[1]))\n",
    "print(cohen_kappa_score(raters[1], raters[2]))\n",
    "print(cohen_kappa_score(raters[0], raters[2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "genetic-exemption",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "x_simple = np.array([1, -1, 1])\n",
    "y_simple = np.array([1, 0, -1])\n",
    "my_rho = np.corrcoef(x_simple, y_simple)\n",
    "\n",
    "print(my_rho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "empirical-creator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "####\n",
      "0.9999999999999998\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "def average(x):\n",
    "    assert len(x) > 0\n",
    "    return float(sum(x)) / len(x)\n",
    "\n",
    "def pearson_correlation(x, y):\n",
    "    assert len(x) == len(y)\n",
    "    n = len(x)\n",
    "    assert n > 0\n",
    "    avg_x = average(x)\n",
    "    avg_y = average(y)\n",
    "    diffprod = 0\n",
    "    xdiff2 = 0\n",
    "    ydiff2 = 0\n",
    "    for idx in range(n):\n",
    "        xdiff = x[idx] - avg_x\n",
    "        ydiff = y[idx] - avg_y\n",
    "        diffprod += xdiff * ydiff\n",
    "        xdiff2 += xdiff * xdiff\n",
    "        ydiff2 += ydiff * ydiff\n",
    "\n",
    "    return diffprod / math.sqrt(xdiff2 * ydiff2)\n",
    "\n",
    "print(pearson_correlation(raters[0], raters[1]))\n",
    "print(pearson_correlation(raters[1], raters[2]))\n",
    "print(pearson_correlation(raters[0], raters[2]))\n",
    "print(\"####\")\n",
    "print(np.corrcoef(raters[0], raters[1])[0,1])\n",
    "print(np.corrcoef(raters[1], raters[2])[0,1])\n",
    "print(np.corrcoef(raters[0], raters[2])[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documented-corruption",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "weird-cambridge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File path : C:\\Users\\jason\\OneDrive - University College Dublin\\Documents\\MSc Computer Science\\Final Semester\\COMP47600 - Text Analytics\\Practical 9\\Practical\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "cwd = Path().absolute()\n",
    "__location__ = cwd\n",
    "print(f'File path : {__location__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "junior-vulnerability",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using all words as features\n",
      "train on 7998 instances, test on 2666 instances\n",
      "accuracy: 0.77344336084021\n",
      "pos precision: 0.7881422924901186\n",
      "pos recall: 0.7479369842460615\n",
      "neg precision: 0.7601713062098501\n",
      "neg recall: 0.7989497374343586\n",
      "Most Informative Features\n",
      "              engrossing = True              pos : neg    =     17.0 : 1.0\n",
      "                   quiet = True              pos : neg    =     15.7 : 1.0\n",
      "                mediocre = True              neg : pos    =     13.7 : 1.0\n",
      "               absorbing = True              pos : neg    =     13.0 : 1.0\n",
      "                portrait = True              pos : neg    =     12.4 : 1.0\n",
      "                   flaws = True              pos : neg    =     12.3 : 1.0\n",
      "               inventive = True              pos : neg    =     12.3 : 1.0\n",
      "              refreshing = True              pos : neg    =     12.3 : 1.0\n",
      "            refreshingly = True              pos : neg    =     11.7 : 1.0\n",
      "                 triumph = True              pos : neg    =     11.7 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# http://andybromberg.com/sentiment-analysis-python/\n",
    "# Andy Bromberg's Simple Sentiment Analysis System\n",
    "# Uses data from Pang & Lee (2005)\n",
    "# Uses a Naive Bayes Classifier Train the System\n",
    "#  NB Updated 2016 for package changes around scores\n",
    "\n",
    "import re, math, collections, itertools, sys, os\n",
    "import nltk, nltk.classify.util, nltk.metrics\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.metrics import BigramAssocMeasures, scores\n",
    "from nltk.probability import FreqDist, ConditionalFreqDist\n",
    "\n",
    "# __location__ = os.path.realpath(os.path.join(os.getcwd(), os.path.dirname(__file__)))\n",
    "\n",
    "\n",
    "def evaluate_features(feature_select):\n",
    "    #reading pre-labeled input and splitting into lines\n",
    "    negSentences = open(os.path.join(__location__, 'rt-polarity-neg.txt'), 'r', encoding='utf8')\n",
    "    posSentences = open(os.path.join(__location__, 'rt-polarity-pos.txt'), 'r', encoding='utf8')\n",
    "    negSentences = re.split(r'\\n', negSentences.read())\n",
    "    posSentences = re.split(r'\\n', posSentences.read())\n",
    "    \n",
    "    posFeatures = []\n",
    "    negFeatures = []\n",
    "    # breaks up the sentences into lists of individual words\n",
    "    # creates instance structures for classifier\n",
    "    for i in posSentences:\n",
    "        posWords = re.findall(r\"[\\w']+|[.,!?;]\", i)\n",
    "        posWords = [feature_select(posWords), 'pos']\n",
    "        posFeatures.append(posWords)\n",
    "    for i in negSentences:\n",
    "        negWords = re.findall(r\"[\\w']+|[.,!?;]\", i)\n",
    "        negWords = [feature_select(negWords), 'neg']\n",
    "        negFeatures.append(negWords)\n",
    "        \n",
    "    posCutoff = int(math.floor(len(posFeatures)*3/4))\n",
    "    negCutoff = int(math.floor(len(negFeatures)*3/4))\n",
    "    trainFeatures = posFeatures[:posCutoff] + negFeatures[:negCutoff]\n",
    "    testFeatures = posFeatures[posCutoff:] + negFeatures[negCutoff:]\n",
    "    \n",
    "    #Runs the classifier on the testFeatures\n",
    "    classifier = NaiveBayesClassifier.train(trainFeatures)\n",
    "    \n",
    "    #Sets up labels to look at output\n",
    "    referenceSets = collections.defaultdict(set)\n",
    "    testSets = collections.defaultdict(set)\n",
    "    for i, (features, label) in enumerate(testFeatures): # enumerate adds number-count to each item\n",
    "        referenceSets[label].add(i)               # recorded polarity for these test sentences\n",
    "        predicted = classifier.classify(features) # classifiers' proposed polarity for tests\n",
    "        testSets[predicted].add(i)\n",
    "\n",
    "    #Outputs\n",
    "    print('train on %s instances, test on %s instances'% (len(trainFeatures), len(testFeatures)))\n",
    "    print('accuracy:', nltk.classify.util.accuracy(classifier, testFeatures))\n",
    "    print('pos precision:', scores.precision(referenceSets['pos'], testSets['pos']))\n",
    "    print('pos recall:', scores.recall(referenceSets['pos'], testSets['pos']))\n",
    "    print('neg precision:', scores.precision(referenceSets['neg'], testSets['neg']))\n",
    "    print('neg recall:', scores.recall(referenceSets['neg'], testSets['neg']))\n",
    "    classifier.show_most_informative_features(10)\n",
    "\n",
    "def make_full_dict(words):\n",
    "    return dict([(word, True) for word in words])\n",
    "\n",
    "print('using all words as features')\n",
    "evaluate_features(make_full_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "wireless-samoa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "file1 = open(\"rt-polarity-pos.txt\", \"r\",  encoding='utf-8')\n",
    "line = file1.read()\n",
    "words = word_tokenize(line)\n",
    "words_witout_stop_words = [\"\" if word in stop_words else word for word in words]\n",
    "new_words = \" \".join(words_witout_stop_words).strip()\n",
    "new_words\n",
    "appendFile = open('filtered-rt-polarity-pos.txt','w', encoding=\"utf-8\")\n",
    "appendFile.write(new_words)\n",
    "appendFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "electrical-efficiency",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "file2 = open(\"rt-polarity-neg.txt\", \"r\",  encoding='utf-8')\n",
    "line = file2.read()\n",
    "words = word_tokenize(line)\n",
    "words_witout_stop_words = [\"\" if word in stop_words else word for word in words]\n",
    "new_words = \" \".join(words_witout_stop_words).strip()\n",
    "new_words\n",
    "appendFile = open('filtered-rt-polarity-neg.txt','w', encoding=\"utf-8\")\n",
    "appendFile.write(new_words)\n",
    "appendFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "acquired-mandate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using all words as features\n",
      "train on 7998 instances, test on 2666 instances\n",
      "accuracy: 0.77344336084021\n",
      "pos precision: 0.7881422924901186\n",
      "pos recall: 0.7479369842460615\n",
      "neg precision: 0.7601713062098501\n",
      "neg recall: 0.7989497374343586\n",
      "Most Informative Features\n",
      "              engrossing = True              pos : neg    =     17.0 : 1.0\n",
      "                   quiet = True              pos : neg    =     15.7 : 1.0\n",
      "                mediocre = True              neg : pos    =     13.7 : 1.0\n",
      "               absorbing = True              pos : neg    =     13.0 : 1.0\n",
      "                portrait = True              pos : neg    =     12.4 : 1.0\n",
      "                   flaws = True              pos : neg    =     12.3 : 1.0\n",
      "               inventive = True              pos : neg    =     12.3 : 1.0\n",
      "              refreshing = True              pos : neg    =     12.3 : 1.0\n",
      "            refreshingly = True              pos : neg    =     11.7 : 1.0\n",
      "                 triumph = True              pos : neg    =     11.7 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# http://andybromberg.com/sentiment-analysis-python/\n",
    "# Andy Bromberg's Simple Sentiment Analysis System\n",
    "# Uses data from Pang & Lee (2005)\n",
    "# Uses a Naive Bayes Classifier Train the System\n",
    "#  NB Updated 2016 for package changes around scores\n",
    "\n",
    "import re, math, collections, itertools, sys, os\n",
    "import nltk, nltk.classify.util, nltk.metrics\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.metrics import BigramAssocMeasures, scores\n",
    "from nltk.probability import FreqDist, ConditionalFreqDist\n",
    "\n",
    "# __location__ = os.path.realpath(os.path.join(os.getcwd(), os.path.dirname(__file__)))\n",
    "\n",
    "\n",
    "def evaluate_features(feature_select):\n",
    "    #reading pre-labeled input and splitting into lines\n",
    "    negSentences = open(os.path.join(__location__, 'rt-polarity-neg.txt'), 'r', encoding='utf8')\n",
    "    posSentences = open(os.path.join(__location__, 'rt-polarity-pos.txt'), 'r', encoding='utf8')\n",
    "    negSentences = re.split(r'\\n', negSentences.read())\n",
    "    posSentences = re.split(r'\\n', posSentences.read())\n",
    "    \n",
    "    posFeatures = []\n",
    "    negFeatures = []\n",
    "    # breaks up the sentences into lists of individual words\n",
    "    # creates instance structures for classifier\n",
    "    for i in posSentences:\n",
    "        posWords = re.findall(r\"[\\w']+|[.,!?;]\", i)\n",
    "        posWords = [feature_select(posWords), 'pos']\n",
    "        posFeatures.append(posWords)\n",
    "    for i in negSentences:\n",
    "        negWords = re.findall(r\"[\\w']+|[.,!?;]\", i)\n",
    "        negWords = [feature_select(negWords), 'neg']\n",
    "        negFeatures.append(negWords)\n",
    "        \n",
    "    posCutoff = int(math.floor(len(posFeatures)*3/4))\n",
    "    negCutoff = int(math.floor(len(negFeatures)*3/4))\n",
    "    trainFeatures = posFeatures[:posCutoff] + negFeatures[:negCutoff]\n",
    "    testFeatures = posFeatures[posCutoff:] + negFeatures[negCutoff:]\n",
    "    \n",
    "    #Runs the classifier on the testFeatures\n",
    "    classifier = NaiveBayesClassifier.train(trainFeatures)\n",
    "    \n",
    "    #Sets up labels to look at output\n",
    "    referenceSets = collections.defaultdict(set)\n",
    "    testSets = collections.defaultdict(set)\n",
    "    for i, (features, label) in enumerate(testFeatures): # enumerate adds number-count to each item\n",
    "        referenceSets[label].add(i)               # recorded polarity for these test sentences\n",
    "        predicted = classifier.classify(features) # classifiers' proposed polarity for tests\n",
    "        testSets[predicted].add(i)\n",
    "\n",
    "    #Outputs\n",
    "    print('train on %s instances, test on %s instances'% (len(trainFeatures), len(testFeatures)))\n",
    "    print('accuracy:', nltk.classify.util.accuracy(classifier, testFeatures))\n",
    "    print('pos precision:', scores.precision(referenceSets['pos'], testSets['pos']))\n",
    "    print('pos recall:', scores.recall(referenceSets['pos'], testSets['pos']))\n",
    "    print('neg precision:', scores.precision(referenceSets['neg'], testSets['neg']))\n",
    "    print('neg recall:', scores.recall(referenceSets['neg'], testSets['neg']))\n",
    "    classifier.show_most_informative_features(10)\n",
    "\n",
    "def make_full_dict(words):\n",
    "    return dict([(word, True) for word in words])\n",
    "\n",
    "print('using all words as features')\n",
    "evaluate_features(make_full_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "seeing-pathology",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stop_words = [\"a\",\n",
    "\"about\",\n",
    "\"above\",\n",
    "\"after\",\n",
    "\"again\",\n",
    "\"against\",\n",
    "\"all\",\n",
    "\"am\",\n",
    "\"an\",\n",
    "\"and\",\n",
    "\"any\",\n",
    "\"are\",\n",
    "\"aren't\",\n",
    "\"as\",\n",
    "\"at\",\n",
    "\"be\",\n",
    "\"because\",\n",
    "\"been\",\n",
    "\"before\",\n",
    "\"being\",\n",
    "\"below\",\n",
    "\"between\",\n",
    "\"both\",\n",
    "\"but\",\n",
    "\"by\",\n",
    "\"can't\",\n",
    "\"cannot\",\n",
    "\"could\",\n",
    "\"couldn't\",\n",
    "\"did\",\n",
    "\"didn't\",\n",
    "\"do\",\n",
    "\"does\",\n",
    "\"doesn't\",\n",
    "\"doing\",\n",
    "\"don't\",\n",
    "\"down\",\n",
    "\"during\",\n",
    "\"each\",\n",
    "\"few\",\n",
    "\"for\",\n",
    "\"from\",\n",
    "\"further\",\n",
    "\"had\",\n",
    "\"hadn't\",\n",
    "\"has\",\n",
    "\"hasn't\",\n",
    "\"have\",\n",
    "\"haven't\",\n",
    "\"having\",\n",
    "\"he\",\n",
    "\"he'd\",\n",
    "\"he'll\",\n",
    "\"he's\",\n",
    "\"her\",\n",
    "\"here\",\n",
    "\"here's\",\n",
    "\"hers\",\n",
    "\"herself\",\n",
    "\"him\",\n",
    "\"himself\",\n",
    "\"his\",\n",
    "\"how\",\n",
    "\"how's\",\n",
    "\"i\",\n",
    "\"i'd\",\n",
    "\"i'll\",\n",
    "\"i'm\",\n",
    "\"i've\",\n",
    "\"if\",\n",
    "\"in\",\n",
    "\"into\",\n",
    "\"is\",\n",
    "\"isn't\",\n",
    "\"it\",\n",
    "\"it's\",\n",
    "\"its\",\n",
    "\"itself\",\n",
    "\"let's\",\n",
    "\"me\",\n",
    "\"more\",\n",
    "\"most\",\n",
    "\"mustn't\",\n",
    "\"my\",\n",
    "\"myself\",\n",
    "\"no\",\n",
    "\"nor\",\n",
    "\"not\",\n",
    "\"of\",\n",
    "\"off\",\n",
    "\"on\",\n",
    "\"once\",\n",
    "\"only\",\n",
    "\"or\",\n",
    "\"other\",\n",
    "\"ought\",\n",
    "\"our\",\n",
    "\"ours\",\n",
    "\"out\",\n",
    "\"over\",\n",
    "\"own\",\n",
    "\"same\",\n",
    "\"shan't\",\n",
    "\"she\",\n",
    "\"she'd\",\n",
    "\"she'll\",\n",
    "\"she's\",\n",
    "\"should\",\n",
    "\"shouldn't\",\n",
    "\"so\",\n",
    "\"some\",\n",
    "\"such\",\n",
    "\"than\",\n",
    "\"that\",\n",
    "\"that's\",\n",
    "\"the\",\n",
    "\"their\",\n",
    "\"theirs\",\n",
    "\"them\",\n",
    "\"themselves\",\n",
    "\"then\",\n",
    "\"there\",\n",
    "\"there's\",\n",
    "\"these\",\n",
    "\"they\",\n",
    "\"they'd\",\n",
    "\"they'll\",\n",
    "\"they're\",\n",
    "\"they've\",\n",
    "\"this\",\n",
    "\"those\",\n",
    "\"through\",\n",
    "\"to\",\n",
    "\"too\",\n",
    "\"under\",\n",
    "\"until\",\n",
    "\"up\",\n",
    "\"very\",\n",
    "\"was\",\n",
    "\"wasn't\",\n",
    "\"we\",\n",
    "\"we'd\",\n",
    "\"we'll\",\n",
    "\"we're\",\n",
    "\"we've\",\n",
    "\"were\",\n",
    "\"weren't\",\n",
    "\"what\",\n",
    "\"what's\",\n",
    "\"when\",\n",
    "\"when's\",\n",
    "\"where\",\n",
    "\"where's\",\n",
    "\"which\",\n",
    "\"while\",\n",
    "\"who\",\n",
    "\"who's\",\n",
    "\"whom\",\n",
    "\"why\",\n",
    "\"why's\",\n",
    "\"with\",\n",
    "\"won't\",\n",
    "\"would\",\n",
    "\"wouldn't\",\n",
    "\"you\",\n",
    "\"you'd\",\n",
    "\"you'll\",\n",
    "\"you're\",\n",
    "\"you've\",\n",
    "\"your\",\n",
    "\"yours\",\n",
    "\"yourself\",\n",
    "\"yourselves\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "absent-traffic",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xf3 in position 619979: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-dbd60967ff1e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mfile2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"filtered-rt-polarity-neg.txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# Use this to read file content as a stream:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfile2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\wheelie\\lib\\codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;31m# decode input (taking the buffer into account)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 322\u001b[1;33m         \u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconsumed\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    323\u001b[0m         \u001b[1;31m# keep undecoded input until the next call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mconsumed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xf3 in position 619979: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "file2 = open(\"filtered-rt-polarity-neg.txt\", \"r\",  encoding='utf-8')\n",
    "# Use this to read file content as a stream:\n",
    "line = file2.read()\n",
    "words = line.split()\n",
    "for r in words:\n",
    "    if not r in english_stop_words:\n",
    "        appendFile = open('filtered-rt-polarity-neg.txt','a',encoding='utf-8' )\n",
    "        appendFile.write(\" \"+r)\n",
    "        appendFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "skilled-healthcare",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rough-edinburgh",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
