# Understand and perform K-L divergence

### Tasks Performed:

1. K-L divergence in Python

2. Understand how one probability distribution(PD) of doc 'i' is different from the reference PD of 'a'
